---
layout: post
title:  "人工智能安全与对齐"
author: "Sam Altman"
date:   2023-06-10 12:00:00 +0800
image:  https://i.imgur.com/scxsBgP.png
#image_caption: ""
description:   "编译自2023年6月10日OpenAI创始人Sam Altman首次对中国观众发表视频连线演讲。"
position: right
---

随着人工智能日益强大，全球合作从未如此重要。我们需要共同努力，寻找共性来推进通用人工智能（AGI）安全。

<!--more-->

如果我们不小心，一个旨在改善公共卫生结果的错位的人工智能系统，可能会提供没有根据的建议，从而扰乱整个医疗保健系统。同样，为优化农业生产而设计的人工智能系统，可能会因为缺乏对影响粮食生产的长期可持续性这种环境平衡的考虑，无意中耗尽自然资源或破坏生态系统。


### AGI治理

AGI从根本上成为了改变我们文明的强大力量，突显出有意义的国际合作和协调的必要性。每个人都会从合作治理方法中受益。如果我们安全、负责任地驾驭这条道路，AGI系统可以为全球经济创造无与伦比的经济繁荣，解决气候变化和全球健康安全等共同挑战，并提高社会福祉。

我也深深地相信在未来，我们需要在AGI安全方面进行投资，才能到达所想要达到的地方，并在那里享受它。

要做到这样我们需要认真协调。这是一项具有全球影响力的全球技术。不计后果的开发和部署所造成的事故成本将影响到我们所有人。

国际合作中，我认为有两个关键领域是最重要的。

首先，我们需要建立国际规范和标准，并注意过程中注重包容性。在任何国家使用AGI系统，都应平等而一致地遵循这样的国际标准和规范。在这些安全护栏内，我们相信人们有足够的机会做出自己的选择。

其次，我们需要国际合作，以可核查的方式建立对安全开发日益强大的AI系统的国际间信任。我并不妄想这是一件容易的事，这需要投入大量和持续的关注。

《道德经》：千里之行，始于足下。我们认为，在这方面最具建设性的第一步是与国际科学和技术界合作。

需要强调的是，我们应该在推动技术进步这一方面增加透明度和知识共享的机制。在AGI安全方面，发现新出现的安全问题的研究人员们应该为了更大的利益分享他们的见解。

我们需要认真思考如何在鼓励这种规范的同时，也尊重和保护知识产权。如果我们这样做，那么，它将为我们深化合作打开新的大门。


### AI对齐和安全研究

更广泛地来讲，我们应该投资于促进和引导对AI对齐和安全的研究。

在OpenAI，我们今天的研究主要集中在技术问题上，让AI在我们目前的系统中充当一个有帮助且更安全的角色。这可能也意味着，我们训练ChatGPT的方式，使其不做出暴力威胁或协助用户进行有害活动的举措。

但随着我们日益接近AGI的时代，没有对齐的AI系统的潜在影响力和影响规模将成倍增长。现在积极主动地解决这些挑战，能将未来出现灾难性结果的风险降到最低。

对于目前的系统，我们主要利用人类反馈进行强化学习来训练模型，使其成为一个有帮助的安全助手。这只是各种训练后调整技术中的一个例子。而且我们也在努力研究新的技术，其中需要很多艰苦的工程工作。

从GPT-4完成预培训到部署，我们专门花了8个月的时间来进行对齐方面的工作。总的来说，我们认为我们在这方面做得很好。GPT-4比我们以前的任何模型都更加与人类对齐。

然而，对于更先进的系统，对齐仍是个未解决的问题，我们认为这需要新的技术方法，同时增强治理和监督。

未来的AGI系统，可能会有10万行二进制代码。人类监督者不太可能发现这样的模型是否在做一些邪恶的事情。所以我们正在投资一些新的、互补的研究方向，希望能够实现突破。

一个是可扩展监督。我们可以尝试使用人工智能系统来协助人类监督其他人工智能系统。例如，我们可以训练一个模型来帮助人类监督发现其他模型的输出中的缺陷。

另一个是解释能力。我们想尝试更好地了解这些模型内部发生了什么。我们最近发表了一篇论文，使用GPT-4来解释GPT-2中的神经元。在另一篇论文中，我们使用Model Internals来检测一个模型何时在说谎。我们还有很长的路要走。我们相信，先进的机器学习技术可以进一步提高我们解释的能力。

最终，我们的目标是训练AI系统来帮助进行对齐研究。这种方法的好处是可以随着AI的发展速度而扩展。

获得AGI带来的非凡好处，并同时降低风险，是我们这个时代的开创性挑战之一。我们看到中国、美国以及世界各地的研究人员有很大的潜力来共同实现同一个目标，并致力于努力解决AGI对齐带来的技术挑战。

如果我们这样做，我相信我们将能够利用AGI来解决世界上最重要的问题，并极大地改善人类的生活质量。

![image1](https://i.imgur.com/3p3Bz3k.jpg)
