<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://agorahub.github.io/pen0/heros.xml" rel="self" type="application/atom+xml" /><link href="https://agorahub.github.io/pen0/" rel="alternate" type="text/html" /><updated>2025-08-14T16:44:35+08:00</updated><id>https://agorahub.github.io/pen0/heros.xml</id><title type="html">The Republic of Agora | Heros</title><subtitle>UNITE THE PUBLIC ♢ VOL.55 © MMXXV</subtitle><entry><title type="html">All AI Models Might Be The Same</title><link href="https://agorahub.github.io/pen0/heros/2025-07-18-JackMorris-a1_c-all-ai-models-might-be-the-same.html" rel="alternate" type="text/html" title="All AI Models Might Be The Same" /><published>2025-07-18T12:00:00+08:00</published><updated>2025-07-18T12:00:00+08:00</updated><id>https://agorahub.github.io/pen0/heros/JackMorris-a1_c-all-ai-models-might-be-the-same</id><content type="html" xml:base="https://agorahub.github.io/pen0/heros/2025-07-18-JackMorris-a1_c-all-ai-models-might-be-the-same.html">&lt;p&gt;&lt;em&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;What can the AI language model embeddings tell us about understanding whale speech and decrypting ancient texts?&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Nc6lDia.png&quot; alt=&quot;image01&quot; /&gt;
&lt;em&gt;▲ &lt;a href=&quot;https://www.projectceti.org/&quot;&gt;Project CETI&lt;/a&gt; is a large-scale effort to decode whale speech. If AI models do learn a universal language, we might be able to use it to talk to whales.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Growing up, I sometimes played a game with my friends called “Mussolini or Bread.”&lt;/p&gt;

&lt;p&gt;It’s a guessing game, kind of like Twenty Questions. The funny name comes from the idea that, in the space of everything, “Mussolini” and “bread” are about as far away from each other as you can get.&lt;/p&gt;

&lt;p&gt;One round might go like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Is it closer to Mussolini or bread? &lt;em&gt;Mussolini.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is it closer to Mussolini or David Beckham? &lt;em&gt;Uhh, I guess Mussolini.&lt;/em&gt; (Ok, they’re definitely thinking of a person.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is it closer to Mussolini or Bill Clinton? &lt;em&gt;Bill Clinton.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is it closer to Bill Clinton or Pelé? &lt;em&gt;Bill Clinton, I think.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is it closer to Bill Clinton or Grace Hopper? &lt;em&gt;Grace Hopper.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is it closer to Grace Hopper or Richard Hamming? &lt;em&gt;Richard Hamming.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is it closer to Richard Hamming or Claude Shannon? &lt;em&gt;You got it, I was thinking of Claude Shannon.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hopefully you get the point. By successively narrowing down the space of possible things or people, we’re able to guess almost anything.&lt;/p&gt;

&lt;p&gt;How is this game possible? Mussolini or Bread only works because you and I have a shared sense of semantics. Before we played this game, we never talked about whether Claude Shannon is semantically “closer” to Mussolini or Beckham. We never even talked about what it means for two things to be “close”, even, or agreed on rules to the game.&lt;/p&gt;

&lt;p&gt;As you might imagine, the edge cases in M or B can be controversial. But I’ve played this game with many people and people tend to “just get it” on their first try. How is that possible?&lt;/p&gt;

&lt;h3 id=&quot;a-universal-sense-of-semantics&quot;&gt;A universal sense of semantics&lt;/h3&gt;

&lt;p&gt;One explanation for why this game works is that there is only one way in which things are related, and this comes from the underlying world we live in. Put another way, our brains build up complicated models of the world in which we live, and the model of the world that my brain relies on is very similar to the one in yours. In fact, our brains’ models of the world are so similar that we can narrow down almost any concept by successively refining the questions we ask, a-la Mussolini or Bread.&lt;/p&gt;

&lt;p&gt;Let’s try to explain this through the lens of compression. One perspective on AI is that we’re just learning to compress all the data in the world. In fact, the task of language modeling (predicting the next word) can be seen as a compression task, ever since &lt;a href=&quot;https://en.wikipedia.org/wiki/Shannon's_source_coding_theorem&quot;&gt;Shannon’s source coding theorem&lt;/a&gt; formalized the relationship between probability distributions and compression algorithms.&lt;/p&gt;

&lt;p&gt;In recent years, we’ve developed much more accurate probability distributions of the world; this turned out to be easy, since &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;bigger and bigger language models&lt;/a&gt; give us &lt;a href=&quot;https://arxiv.org/abs/1712.00409&quot;&gt;better and better probability distributions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/vA7jkBg.jpeg&quot; alt=&quot;image02&quot; /&gt;
&lt;em&gt;▲ Intelligence is compression, and compression follows scaling laws.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;And with better probability distributions comes better compression. In practice, we find that a model that can compress real data better knows more about the world. And thus there is a duality between compression and intelligence. Compression is intelligence. Some have even said &lt;a href=&quot;https://www.youtube.com/watch?v=dO4TPJkeaaU&quot;&gt;compression may be the way to AGI&lt;/a&gt;. Ilya gave a &lt;a href=&quot;https://www.lesswrong.com/posts/KqgujtM3vSAfZE2dR/on-ilya-sutskever-s-a-theory-of-unsupervised-learning#Kolmogorov_Complexity__20_48_&quot;&gt;famously incomprehensible talk&lt;/a&gt; about the connections between intelligence and compression.&lt;/p&gt;

&lt;p&gt;Last year some folks at DeepMind wrote a paper simply titled &lt;a href=&quot;https://arxiv.org/abs/2309.10668&quot;&gt;Language Modeling Is Compression&lt;/a&gt; and actually tested different language models’ ability to compress various data modalities. Across the board, they found that smarter language models are better compressors. (Of course, this is what we’d expect, given the source coding theorem.)&lt;/p&gt;

&lt;p&gt;And learning to compress is exactly how models end up generalizing. Some of &lt;a href=&quot;https://arxiv.org/abs/2505.24832&quot;&gt;our recent work&lt;/a&gt; has analyzed models’ compression behavior in the limit of training: we train models for infinitely long on datasets of varying size.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/sO4ryEK.jpeg&quot; alt=&quot;image03&quot; /&gt;
&lt;em&gt;▲ Figures from our recent work, &lt;a href=&quot;https://arxiv.org/abs/2505.24832&quot;&gt;How much can language models memorize?&lt;/a&gt; Generalization only begins when compression is no longer possible, since the model can’t store data points separately and is forced to combine things.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When a model can fit the training dataset perfectly (left side of both graphs) we see that it memorizes data really well, and totally fails to generalize. But when the dataset gets too big, and the model can no longer fit all of the data in its parameters, it’s forced to “combine” information from multiple datapoints in order to get the best training loss. This is where generalization occurs.&lt;/p&gt;

&lt;p&gt;And the central idea I’ll push here is that when generalization occurs, it usually occurs in the same way, even within different models. From the compression perspective, under a given architecture and within a fixed number of parameters, there is only one way to compress the data well. This sounds like a crazy idea–and it is– but across different domains and models, there turns out to be a lot of evidence for this phenomenon.&lt;/p&gt;

&lt;h3 id=&quot;the-platonic-representation-hypothesis&quot;&gt;The Platonic Representation Hypothesis&lt;/h3&gt;

&lt;p&gt;So how can different models be learning the shared representations? Given &lt;a href=&quot;https://transformer-circuits.pub/2023/privileged-basis/index.html&quot;&gt;the massive number of ~equivalent ways in which a model can represent things&lt;/a&gt;, why should two models ever converge to analogous representations?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ydhENlm.jpeg&quot; alt=&quot;image04&quot; /&gt;
&lt;em&gt;▲ A terse description and illustration of the headlining theory from &lt;a href=&quot;https://arxiv.org/abs/2405.07987&quot;&gt;The Platonic Representation Hypothesis&lt;/a&gt; (2024).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Remember what these models are really doing is modeling the relationships between things in the world. In some sense there’s only one correct way to model things, and that’s the true model, the one that perfectly reflects the reality in which we live. Perhaps an infinitely large model with infinite training data would be a perfect simulator of the world itself.&lt;/p&gt;

&lt;p&gt;As models have gotten bigger, their similarities have become more apparent. The theory that models are converging to a shared underlying representation space was formalized in The Platonic Representation Hypothesis, &lt;a href=&quot;https://arxiv.org/abs/2405.07987&quot;&gt;a position paper written by a group of MIT researchers in 2024&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/sesczY7.jpeg&quot; alt=&quot;image05&quot; /&gt;
&lt;em&gt;▲ The Platonic Representation Hypothesis argues that as models get bigger, they’re learning more and more of the same features. They provide evidence for this in vision and language.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Platonic Representation Hypothesis argues that models are converging to a shared representation space, and this is becoming more true as we make models bigger and smarter. This is true in text and language, at a minimum,&lt;/p&gt;

&lt;p&gt;Remember &lt;a href=&quot;https://situational-awareness.ai/&quot;&gt;the trends in scaling&lt;/a&gt; show that models are getting all three of bigger, smarter, and more efficient every year. That means that we can expect models to get more similar, too, as the years go on.&lt;/p&gt;

&lt;h3 id=&quot;a-brief-aside-on-embedding-inversion&quot;&gt;A brief aside on embedding inversion&lt;/h3&gt;

&lt;p&gt;The evidence for the Platonic Representation Hypothesis is compelling. But is it useful? Before I explain how to take advantage of the PRH, I have to give a bit of background on a problem of &lt;a href=&quot;https://arxiv.org/abs/2310.06816&quot;&gt;embedding inversion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I worked for a year or so of my PhD on this problem: given a representation vector from a neural network, can we infer what text was inputted to the network?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/cF7I57O.jpeg&quot; alt=&quot;image06&quot; /&gt;
&lt;em&gt;▲ Visualization of a network that reconstructs images astonishingly well given only the 1000 class probability predictions from an ImageNet classifier (from &lt;a href=&quot;https://arxiv.org/abs/2103.07470&quot;&gt;Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We thought inversion should be possible because &lt;a href=&quot;https://arxiv.org/abs/2103.07470&quot;&gt;results on ImageNet&lt;/a&gt; showed that they could do very effective reconstruction given only a model’s output of 1000 class probabilities. This is extremely unintuitive. Apparently knowing that an image is 0.0001% parakeet and 0.0017% baboon is useful enough to infer not only the true class but lots of irrelevant information like facial structure, pose, and background details.&lt;/p&gt;

&lt;p&gt;In the realm of text, the problem looks easy on its face, because typical embedding vectors have ~1000 floating-point numbers in them, or around 16 KB of data. If you store 16KB of text, it can represent quite a lot. Since we were working with datapoints on the level of long sentences or short documents, it seemed reasonable that we would be able to do inversion quite well.&lt;/p&gt;

&lt;p&gt;But it turns out to be really hard. This mostly comes about because embeddings are in some sense extremely compressed: since similar texts have similar embeddings, it becomes very difficult to distinguish between two embeddings that similar-but-different data. So our models could output something close to the embedding, but almost never the exactly-correct text.&lt;/p&gt;

&lt;p&gt;We ended up getting around this problem by using a primitive form of test-time compute: we made many queries to the embedding space and built a model that could “narrow down” the true text by iteratively improving itself in embedding space. Our system looks kind of like a learned optimizer that takes text-based steps to move position in embedding space.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/fdgeWlJ.jpeg&quot; alt=&quot;image07&quot; /&gt;
&lt;em&gt;▲ Iterative refinement is an extremely effective method for embedding inversion (&lt;a href=&quot;https://thegradient.pub/text-embedding-inversion/&quot;&gt;read more here&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This new approach turns out to work very well. Given an embedding model, we were able to invert text at the level of a long sentence with 94% exact accuracy.&lt;/p&gt;

&lt;h3 id=&quot;harnessing-plato-for-embedding-inversion&quot;&gt;Harnessing Plato for embedding inversion&lt;/h3&gt;

&lt;p&gt;We were very pleased with ourselves after making that method work. This had a whole lot of implications for the new model of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_database&quot;&gt;vector database&lt;/a&gt;: sharing vectors, apparently, is equivalent to sharing the text those vectors represent.&lt;/p&gt;

&lt;p&gt;But unfortunately our method was embedding-specific. It wasn’t clear that it could transfer to future embedding models or private fine-tunes that we didn’t have access to. And it required making a lot of queries to the embedding model we knew: training the models took millions of embeddings.&lt;/p&gt;

&lt;p&gt;We thought that this shouldn’t be the case. If the Platonic Representation Hypothesis is true, and different models (in some sense) are learning the same thing, we should be able to build one universal embedding inverter and use it for any kind of model. This idea set us off on a multi-year quest to “harness” the PRH and build a universal embedding inverter.&lt;/p&gt;

&lt;p&gt;We started by expressing our problem as a mathematical one. Given a bunch of embeddings from model A, and a bunch of embeddings from model B, can we learn to map from A→B (or B→A)?&lt;/p&gt;

&lt;p&gt;Importantly, we don’t have any correspondence, i.e. pairs of texts with representations in both A and B. That’s why this problem is hard. We want to learn to align the spaces of A and B in some way so that we can “magically” learn how to convert between their spaces.&lt;/p&gt;

&lt;p&gt;We realized after a while that this problem has been solved at least once in the deep learning world: work on a model called CycleGAN proposed a way to translate between spaces without correspondence using a method called cycle consistency:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/HWL6xOi.jpeg&quot; alt=&quot;image08&quot; /&gt;
&lt;em&gt;▲ Unpaired image translations from &lt;a href=&quot;https://arxiv.org/abs/1703.10593&quot;&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt; (2017). If you squint, you might see extremely preliminary evidence for the Platonic Representation Hypothesis.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just imagine that the horses and zebras above are a piece of text from model A being translated into the space of model B and back. If this works for zebras and horses, why shouldn’t it work for text?&lt;/p&gt;

&lt;p&gt;And, after at least a year of ruthlessly debugging our own embedding-specific version of CycleGAN, we started to see signs of life. In our unsupervised matching task we started to produce GIFs like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/w4jnFZD.gif&quot; alt=&quot;image09&quot; /&gt;
&lt;em&gt;▲ After training a CycleGAN-like model for mapping between embedding spaces, vec2vec learns to “magically” align them. Hooray for the Platonic Representation Hypothesis!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To us, this was an incredible step forward, and proof for an even stronger claim we call the “Strong Platonic Representation Hypothesis”. Models’ representations share so much structure that we can translate between them, even without having knowledge of individual points in either of the spaces. This meant that we could do unsupervised conversion between models, as well as invert embeddings mined from databases where we know nothing about the underlying model.&lt;/p&gt;

&lt;h3 id=&quot;universality-in-circuits&quot;&gt;Universality in Circuits&lt;/h3&gt;

&lt;p&gt;Some additional evidence for the PRH comes from the world of mechanistic interpretability, where researchers attempt to reverse-engineer the inner workings of models. Work on &lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/#claim-3&quot;&gt;Circuits&lt;/a&gt; in 2020 found very similar functionalities in very different models:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Cj1GITv.jpeg&quot; alt=&quot;image10&quot; /&gt;
&lt;em&gt;▲ Universal feature dectors from Circuits (2020). Different networks exhibit remarkably similar behaviors.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;More recently, there’s been some action around a method for feature discretization known as sparse autoencoders (SAEs). SAEs take a bunch of embeddings and learn a dictionary of interpretable features that can reproduce those embeddings with minimal loss.&lt;/p&gt;

&lt;p&gt;Many are observing that if you train SAEs on two different models, they often learn many of the same features. There’s even been some recent work on “unsupervised concept discovery”, a suite of methods that can compare two SAEs to find feature overlap:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/W7LGbI5.jpeg&quot; alt=&quot;image11&quot; /&gt;
&lt;em&gt;▲ Universal features from Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment (2025).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since the PRH conjectures that models become more aligned as they get stronger, I suspect this type of common circuit discovery will only grow more common.&lt;/p&gt;

&lt;h3 id=&quot;what-can-we-make-of-all-this&quot;&gt;What can we make of all this?&lt;/h3&gt;

&lt;p&gt;Besides being a deep philosophical idea, the Platonic Representation Hypothesis turns out to be an important practical insight with real-world implications. As the mechanistic interpretability community develops better tools for reverse-engineering models, I expect them to find more and more similarities; as models get bigger, this will become more common.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/YUHT5ht.jpeg&quot; alt=&quot;image12&quot; /&gt;
&lt;em&gt;▲ &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_A&quot;&gt;Linear A&lt;/a&gt; is an ancient Greek text that humans have never been able to decrypt. Perhaps the Platonic Representation Hypothesis gives us hope for one day decoding it back to English.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As for our method (vec2vec), we found strong evidence, but things are still brittle. It seems clear that we can learn an unsupervised mapping between text-based models that are trained on the Internet, as well as &lt;a href=&quot;https://openai.com/index/clip/&quot;&gt;CLIP-like&lt;/a&gt; image-text embeddings.&lt;/p&gt;

&lt;p&gt;It’s not obvious whether we can map between languages with high fidelity. If this turns out to be true, we may be able to decode ancient texts such as Linear A or convert whale speech back to a human language. Only time will tell.&lt;/p&gt;</content><author><name>Jack Morris</name></author><summary type="html">What can the AI language model embeddings tell us about understanding whale speech and decrypting ancient texts?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/nTmt0Vp.png" /><media:content medium="image" url="https://i.imgur.com/nTmt0Vp.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">你而家喺邊？</title><link href="https://agorahub.github.io/pen0/heros/2024-04-21-InitiumMedia-a1_l-where-are-you-now-1.html" rel="alternate" type="text/html" title="你而家喺邊？" /><published>2024-04-21T12:00:00+08:00</published><updated>2024-04-21T12:00:00+08:00</updated><id>https://agorahub.github.io/pen0/heros/InitiumMedia-a1_l-where-are-you-now-1</id><content type="html" xml:base="https://agorahub.github.io/pen0/heros/2024-04-21-InitiumMedia-a1_l-where-are-you-now-1.html">&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;一切都是取捨，不會有任何一個KOL可以給你標準答案。&lt;/code&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;自由是刮風時仍可走自己的路&quot;&gt;自由是刮風時仍可走自己的路&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;夏水，英國中部&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;比起說什麼，不說什麼（特別是廣東話）反而是常態。&lt;/p&gt;

&lt;p&gt;離港逾兩年，漸漸習慣英式生活：保持距離。沒有擠迫車廂、無處可逃升降機與熙來攘往大街，天地廣闊，容你選擇不同昔日的生活方式。家母堅持繼續與港人圈子圍爐問暖，我倆口子更傾向默默過日子。不鄭重其事，便能若無其事。&lt;/p&gt;

&lt;p&gt;我們大概不屬「主流」的移民家庭：沒有子女、沒樓可賣、手上既無百萬儲備也沒有COSTCO會籍。花大半年在網上搜集資料，最終選了一個從未到訪的英國中部城巿落腳，托賴很快找到了本行工作，不富貴也餓不死。&lt;/p&gt;

&lt;p&gt;有人說這是「出走的勇氣」。其實，選擇留下打工結婚、買樓生子一樣要莫大勇氣。只是剛好，我選擇迴避留下的不安心，你選擇迴避起身的不安穩，沒有一個決定會被所有人祝福，然後都要勇氣面對後續的悲喜。&lt;/p&gt;

&lt;p&gt;離開的理由很簡單——因為感到噁心與絕望，在這裡剩下付出沉重代價或自我放棄的選項，眼前也只有一條「由治及興」的路。但我特別討厭假裝正常過日子，也不再想把「無能為力」修飾成「順其自然」。唯有出走，才能讓自己從日復日的、打氣又洩氣的生活中改變。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/6eO0IsN.jpeg&quot; alt=&quot;image01&quot; /&gt;
&lt;em&gt;▲ 攝影：夏水&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;但脫離會讓你發現自己的無力與無知。多年生活在高效高薪、可以用錢將責任外判的香港，掩藏了自身的軟弱，事實是風險來臨時，你可能連最基本自保的能力也缺乏。從煮飯、駕駛到語言能力與獨立思考，在異土都強行被放大檢視，最終迫使你重新思考生活的定義、重新認識自己並作出取捨。&lt;/p&gt;

&lt;p&gt;就說我們常感自豪的「效率」好了，在我的職場裡，根本沒有人對此感到驕傲，反而更看重生活平衡。上班日常經常都衝擊我的既有思維：沒有打卡機制，只在門旁放一本簽到簿，大家又出奇誠實地記下自己的上下班時間，哪怕已遲到45分鐘；上班半小時以後同事忽然揹起背包，留下一句「這裏暖氣不夠，我還是回家工作比較舒服」就不見了；忽現發現老闆在家工作，原來是跟小孩約好遛狗，「嘛，反正用Teams（註：視訊工具）開會也可以。」&lt;/p&gt;

&lt;p&gt;更經典的是聖誕前收到同事電郵，大意如下：「各位，本人即將於聖誕清假，並將關上公司電話，別試圖找我，如你有任何有緊急需要（雖然我不認為年尾會有什麼急事），請先找上司某某，她能幫你就好，不能也無辧法，這是人生。」&lt;/p&gt;

&lt;p&gt;This is just life.簡直是顛倒三觀、清理各位港式「黎生」奴性的至理真言。（編按：2019年反修例運動期間，網民呼籲「三罷」爭取五大訴求，有電視記者採訪到一名黎姓市民的想法，他不滿示威並說：「我嘅訴求就係想返工，冇其他。」&lt;/p&gt;

&lt;p&gt;於是慢慢學懂了放慢過活：哪怕是老闆電郵，一律將回覆時間單位由「分鐘」調節為「小時」；除非明說要即日交貨，否則所有電郵上列明的「urgent」與「soon」，全數定性為一至兩週內慢慢處理；請假再也不必胡謅理由，擺上姨媽姑姐紅白二事，就坦蕩蕩的說我要休息連放兩週，還得到上司教路到克羅地亞哪個小鎮旅行最開心。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/hyzLLaA.png&quot; alt=&quot;image02&quot; /&gt;
&lt;em&gt;▲ 攝影：夏水&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;這些是否英式虛偽？自問功力未足以看透。「別亂說，我在倫敦日日忙呀。」當然呀，以上不過是我的片面觀察而非職場之全部，但正如倫敦的一套，也不該是英國的全部。&lt;/p&gt;

&lt;p&gt;當工作不太重視——或說不介意——是否高效，當然有一定程度的「代價」：家母最討厭外出吃飯，等個45分鐘食物都未見影蹤，「真係等到胃痛」。結果有次不幸言中到醫院看急症，從下午5時等到凌晨3時就為了一幅心電圖，氣若游絲的她仍不忙說句「我喺香港邊洗等咁耐！（我在香港才不會等那麼久）雖然她已經忘了自己在香港公立醫院內說過相似的話，只是當時她說「在私家醫院邊洗等咁耐（我在私家醫院才不會等那麼久）」。&lt;/p&gt;

&lt;p&gt;對於習慣以錢換時間、買服務的人來說，這種樓下沒有商場、餓了叫不到三哥米線、凌晨沒有便利店給你衣食、水喉馬桶壞了也無法立即叫人來修的日子當然不好過，房間再大、花園再廣闊、言論選舉再自由也無補於事。&lt;/p&gt;

&lt;p&gt;一切都是取捨，不會有任何一個KOL可以給你標準答案。&lt;/p&gt;

&lt;p&gt;對我來說，哪怕是人到中年，我仍願意用低慾望來換取最大的自由。煮飯艱難但有人願意分甘同味，口音混亂但有人樂意雞同鴨講。常掛在口邊的「呼吸自由空氣」太宏大，我眼中的自由比較純粹：刮風時仍可走自己的路，飄雨時仍可散自己的步，下雪時也無妨仆自己的街。&lt;/p&gt;

&lt;p&gt;這種自由，不繽紛，但開心。&lt;/p&gt;

&lt;h3 id=&quot;像個正常的年輕人一樣思考未來&quot;&gt;像個正常的年輕人一樣思考未來&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Peter，加拿大多倫多&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;早上7時多，終於把響了好幾次的鬧鐘按停。刷牙、煲水、沖茶包、做早餐，然後走10分鐘去搭地鐵上班……似乎無論在香港還是多倫多，打工仔的早上也是一個模樣。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/fkXhlj3.jpeg&quot; alt=&quot;image03&quot; /&gt;
&lt;em&gt;▲ 攝影：Peter&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;2023年7月我才第一次踏足加拿大。坐上飛機之前，我對這個國家僅有幾個模棱兩可的印象——楓葉、冰上曲棍球、冬天很冷、還有人們似乎很喜歡說Sorry——但我卻想成為這片陌生土地的永久居民。不知道其他趕來搭「救生艇」的香港年輕人是否一樣。&lt;/p&gt;

&lt;p&gt;所謂「救生艇」，即是香港遭逢反修例及國安法等巨變後，加國政府給予指定香港大專院校畢業生的特惠移民政策。合乎資格者只需要在加拿大就讀至少一年的專上課程，或者累積一年的工作經驗，就足以申請永居。因為負擔不起學費，所以打工就是我的唯一出路。&lt;/p&gt;

&lt;p&gt;到埗一個月，幸好遇上一家電訊公司的華人代理需要同時懂得英文、廣東話及國語的推銷員——這不正是為香港人度身訂做的職位嗎？果然，應徵之後，我就發現不論是同事，又或者隔壁舖的「行家」，都是香港來的年輕人。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/H9Lnepu.jpeg&quot; alt=&quot;image04&quot; /&gt;
&lt;em&gt;▲ 攝影：Peter&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;如果在香港，一堆20多歲的年輕人聚在一起，應該會有數之不盡的話題吧？但我和朋友們都發現，來了多倫多之後，大家聊天都離不開柴米油鹽。多倫多或許是一個和香港截然不同的城市，但「通脹」和「房價」絕對是兩者的共同語言。&lt;/p&gt;

&lt;p&gt;單論租金，我跟我的室友兩人去年夏天找房子時，地產中介便提醒我們市場競爭激烈。如果剛到埗且未有穩定收入，租客們往往需要預繳半年至一年的租金才能搶到心儀的租盤；地鐵站步行距離內的一房單位，月租叫價大約2500加元（港幣14200元左右）。以多倫多所在安太略省的最低工資（時薪16.55加元）來算，租金已經相當於基層員工（如我）的整份月薪2600加元，幸好我們是兩人合租。&lt;/p&gt;

&lt;p&gt;百物騰貴之外，身為電話服務推銷員，我每天直視的，還有加拿大的數碼鴻溝。為了節省成本，加國三大電訊寡頭都積極鼓勵顧客上網管理自己的電話服務。定位愈廉價的品牌及計劃，就愈仰賴用戶自行解決他們的疑難，有些甚至不設客戶服務熱線；但正正是對智能手機一竅不通的老一輩，才最常使用最便宜的套餐。&lt;/p&gt;

&lt;p&gt;如果遇上了問題又沒有家人協助，他們唯一的希望就是領著最低工資和微薄佣金的店員，例如我；不過老闆當然會希望員工替店鋪先收幾十元服務費才為客人服務。有時遇到付不起錢的人，我就只能夠趁沒人看到，做到就盡做。想當然，一兩個善意的舉動不足以改善一整代人遇到的問題，但至少令我感覺良好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/wfHXSqj.jpeg&quot; alt=&quot;image05&quot; /&gt;
&lt;em&gt;▲ 攝影：Peter&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;到底是什麼令我要拋低香港的家人、朋友、其實還挺喜歡的記者工作，到這個幾乎完全陌生的城市替老人們搞懂手機？自問自己不是家園受戰火摧殘的難民，離鄉背井去做一份收入不高的工作，似乎說不過去。&lt;/p&gt;

&lt;p&gt;我暫時發掘到的答案是：也許因為與香港令人窒息的氛圍保有些許距離，我終於可以脫離「做多一天算一天」的心態，可以像個正常的25歲年輕人一樣，思考未來自己想要過怎樣的生活。也許我還是想回到香港陪伴我的家人朋友，也許我最後會愛上加拿大的生活，但起碼我知道選擇權仍在我手，我仍然感覺是自由的人。&lt;/p&gt;

&lt;h3 id=&quot;肌肉記憶深處的車仔麵之味&quot;&gt;肌肉記憶深處的車仔麵之味&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;K博，台北&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我竟然懷念起屋邨樓下的那碗車仔麵。&lt;/p&gt;

&lt;p&gt;離開香港之後，我再也沒機會在凌晨兩點買一碗熱騰騰的車仔麵了。在香港，我曾經積極參與各種草根社區行動組織，常常和夥伴通宵達旦討論行動策略。從旺角搭通宵小巴回到沙田，下車不遠處就是屋邨的車仔麵檔。&lt;/p&gt;

&lt;p&gt;老街坊說，麵檔老闆做了幾十年，只是檔口換過不少地方。點什麼餸不緊要，靈魂在於那碗湯——應該是魚湯底加上味精和醬油吧，喝下去有點鹹又有點鮮。我很確定有味精，因為吃完通常都很想飲可樂。真是深夜的邪惡食物。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/G02vWU9.jpeg&quot; alt=&quot;image06&quot; /&gt;
&lt;em&gt;▲ 攝影：K博&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;儘管官方論述中的香港文化是什麼「中西薈萃」的「亞洲國際都會」。身為文化研究學者，我視「文化」為生活。見到燦爛的維港、擎天的IFC、饒富意義的獅子山（編按：獅子山分隔九龍和新界，1972年香港電台劇集《獅子山下》由羅文演唱同名主題曲，這首經典歌曲後來被認為象徵一種勵志的香港精神，近20年被政府及各界引用），當然會引起剎那的鄉愁。&lt;/p&gt;

&lt;p&gt;但生活在一個地方，我們不會天天走近這些「地標」，除非你在那邊上班。平常百姓每日觸手可及、深入肌肉記憶深處、交織出一個城市的肌理脈絡的，是那些活躍在大街小巷的平民文化——那些聲音、氣味、空氣的質感、人與物交錯組成的立體動態圖景。&lt;/p&gt;

&lt;p&gt;離開香港後，我在美國生活了四年，第一站是芝加哥。且不說冬天零下10幾20度的日子，光是我居住的社區——犯罪率就令人對「街頭」有點卻步。在芝加哥，有名的街頭食物要數來自美國南方的炸雞。醃製雞肉經過高溫油炸，外脆內嫩多汁，給嚴冬下寒窗苦讀的留學生帶來直達胃腸深處的刺激。&lt;/p&gt;

&lt;p&gt;可是，要買這份靈魂食物，絕不是像我們在香港那樣「閒庭信步」走到樓下五分鐘就能完成的事。我們要開車15分鐘來到炸雞店，店員（以及炸雞）都被保護在一層防彈玻璃後面，現金和炸雞都要通過一道在櫃檯上的旋轉鐵門傳送。炸雞店顧客通常是身材魁梧的男性，作為身材「嬌小」的亞裔，只敢拿了炸雞就匆匆離開。那刻，這份炸雞的份量遠比我在香港買過的所有煎釀三寶都要重，因為這是靠勇氣與信心加持而換來的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/hEAR94f.jpeg&quot; alt=&quot;image07&quot; /&gt;
&lt;em&gt;▲ 攝影：K博&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;後來搬到紐約，我最愛的是在曼哈頓下城的唐人街，孔子大廈旁邊的茶餐廳，吃一碗綿密的艇仔粥和一份有正宗「鹼水味」的牛腩麵。就連臨盆前，我也指定先生去「大永旺」打包一些粥粉麵作為我「最後的晚餐」。那份鄉愁和那份滿足，除了是舌尖上熟悉的味道，也是那個油膩膩的地板、師傅用力斬雞的聲音、用廣東話點餐的吆喝聲、和牆上電視播放著的TVB新聞。&lt;/p&gt;

&lt;p&gt;在那個空間，我彷彿回到了在沙田顯徑的茶餐廳，也回到了兒時在廣州西關的麵店。紐約唐人街，不正正就是這個所謂「省港澳」文化的交匯點嗎？於我，出生於廣州，成長於香港，在紐約唐人街找到了異地的靈魂之鄉，也實在是奇妙。&lt;/p&gt;

&lt;p&gt;如今我在台北居住，生活形態有了重大的改變。我成為了兩個孩子的母親並且要兼顧全職工作，幾乎失去在街頭的生活，雖然說台灣是街頭美食之鄉。說來奇怪，我在美國生活過的幾個城市，到唐人街都能找到味道正宗的香港平民食物，例如粥粉麵之類。但在台灣這個和香港只相隔一小時飛機的地方，卻從未吃到那份熟悉的平民之味。也許是我不夠努力，聽說好幾間正宗的茶餐廳都在西門町，只是我未身體力行去探索。&lt;/p&gt;

&lt;p&gt;但我想，在台灣，更多的是文化的差異。雖說距離近，台灣實屬福佬文化、外省文化和日本文化的合體，而所謂的外省文化，又是江浙以北的飲食文化為主。對於我，香港的味道是那股只能意會難以言傳的「鹼水麵」的味道，是那份糊糊的粟米魚塊，是那杯濃郁的鴛鴦。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/bbGVB8y.jpeg&quot; alt=&quot;image08&quot; /&gt;
&lt;em&gt;▲ 攝影：K博&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;但是這一切，對於福佬文化來說，是有「文化鴻溝」的。雖然近年來台北市出現了不少「港式茶餐廳」，餐廳內甚至不停播放張學友和張國榮，但可惜我暫時未找到一間對味的。當你冠以「港式」頭銜，冥冥中注定是邯鄲學步。因為茶餐廳就是茶餐廳，講求的是那份隨性的個性，是那些舌尖上的味道，而不是單靠牆上的香港風景畫、花階磚地板或播放粵語流行曲就可以建立的。剛從美國搬到台北生活，發現家附近一間很可愛的餐廳叫做「粟米肉粒飯」，很興奮的打算去買午餐，發現裡面根本沒有賣粟米肉粒飯，而店員根本連粟米肉粒飯是什麼都不知道！我心碎了。&lt;/p&gt;

&lt;p&gt;離開香港，更準確的來說，是離開了那個「身份」，那個會通宵達旦與朋暢談，在馳騁的通宵小巴中構想一個更好的城市未來的我。那時候的我，會期待一下車就能買到的車仔麵，是這個不夜城給夜歸人的一聲問候：「食碗麵，沖個涼，早啲唞。」&lt;/p&gt;</content><author><name>端传媒</name></author><summary type="html">一切都是取捨，不會有任何一個KOL可以給你標準答案。</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/cGxGSrO.jpeg" /><media:content medium="image" url="https://i.imgur.com/cGxGSrO.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">《互助论》引言结论</title><link href="https://agorahub.github.io/pen0/heros/1902-10-31-PeterKropotkin-a1_r-mutual-aid-introduction-and-conclusion.html" rel="alternate" type="text/html" title="《互助论》引言结论" /><published>1902-10-31T10:56:15+06:55</published><updated>1902-10-31T10:56:15+06:55</updated><id>https://agorahub.github.io/pen0/heros/PeterKropotkin-a1_r-mutual-aid-introduction-and-conclusion</id><content type="html" xml:base="https://agorahub.github.io/pen0/heros/1902-10-31-PeterKropotkin-a1_r-mutual-aid-introduction-and-conclusion.html">&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;不是以横蛮的暴力和诡诈，而是以互助合作来解释生物和社会的进步。&lt;/code&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;无非是把达尔文学说——生存竞争、权力欲望、适者生存和超人等等——的误解庸俗浮浅地应用到哲学和政治上罢了。&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;引言&quot;&gt;引言&lt;/h3&gt;

&lt;p&gt;我年青时曾旅行于西伯利亚东部和满洲北部，在这些旅行中，动物生活的两个方面给我的印象极深。一个是：大多数动物不得不对残酷的大自然进行的生存竞争的极端严酷性；以及自然力量定期地大规模毁灭生命，结果，在我所考察的广大土地上生物极为稀少。另一个是：即使在动物十分繁盛的几个地方，虽然我竭力寻找，我也从未发现同种动物之间存在着争取生活资料的残酷斗争；而大多数达尔文主义者认为（虽然达尔文本人并不是永远如此）这种斗争是生存竞争的主要特征和进化的主要因素。&lt;/p&gt;

&lt;p&gt;在冬末时节横扫欧亚北部的可怕的暴风雪和往往随之而来的冰霜；在每年5月的下半月，当树上已是花朵盛开、昆虫到处活跃的时候，再次降临的寒霜和暴风雪；早霜和有时在7、8月间突然消灭亿万显虫和草原上的第二窝雏鸟的大雪，8、9月间在温带地区由印度洋的季风带来的暴雨，结果造成仅见于美洲和亚洲东部的大洪水、在高原上使欧洲各国那样大的地区成为泽国；最后，10月初的大雪，最终使得反刍动物在法国加德国那样大的地区绝对不能生活下去，并且成千上万地毁灭了它们——这些就是我在亚洲北部所见到的动物在其中进行生存竞争的环境。它们使我在较早的时期便认识到，在大自然中，达尔文所说的“对过分繁殖的自然遏制”和同种的个体之间为生活资料而进行的斗争比较起来，具有远为重要的意义，同种的个体之间为生活资料而进行的斗争，在一定限度内随处可见，但是它决不能达到前者那样的重要程度。由于生物稀少和人口不足——不是人口过剩——是地球上我们称之为亚洲北部的那一剖分广大地区的显著特征，所以，此后我就十分怀疑（以后的研究证明我的怀疑是正确的），每一个动物的种内是不是真正存在着争取食物和生命的可怕竞争（这是大多数达尔文主义者的一个信条），并且也因而怀疑据说这种竞争在新种的进化中所起的巨大作用。&lt;/p&gt;

&lt;p&gt;另一方面，无论我在哪里看到的动物繁多的地方，例如，有百十种和千百万个动物聚居在一起繁殖子孙的湖泊；啮齿动物的聚居地；当时沿着乌苏里江像在美洲那样大规模迁居的候鸟群；特别是我在黑龙江畔亲眼见到的移居的鹿群，这种聪明的动物在移居时成千上万地从辽阔的地区聚集起来，以便在大雪降临从前奔过黑龙江畔最狭窄的地方——我在从我眼前掠过的这些动物生活情景中所看到的互助和互援竟达到这样的程度，使我认为它在生命的维护和每一个物种的保存并进一步进化中，是最重要的特征。&lt;/p&gt;

&lt;p&gt;最后，我在外贝加里亚的半野生牛群和马群中，在各地的野生反刍动物中以及在松鼠等动物中，发现动物由于上述原因之一必须和缺少食物进行斗争的时候，所有遭受这种灾难的动物，经过这场考验后，全都是那样的体亏力衰，以致物种在如此激烈的竞争时期中是不可能得到任何逐步进化的。&lt;/p&gt;

&lt;p&gt;因此，当我以后开始注意达尔文主义和社会学之间的关系时，没有一本论述这个重要问题的著作和小册子是使我能够同意的。它们全都力图证实人类由于有较高的智慧和知识，因而可以缓和人与人之间生存竞争的严酷性，但是，它们同时又都承认每一个动物和它的同种以及每一个人和所有其他的人为生活资料而进行的竞争，是“一种自然法则”。这种看法是我所不能接受的，因为我以为，承认每一个物种有无情的内部生存竞争，承认这种竞争是进步的一个条件，那就等于承认不仅尚未被证实的、而且缺少直接观察根据的事物。&lt;/p&gt;

&lt;p&gt;相反地，当时圣被得堡大学院长、著名的动物学家凯士勒教授于1880年1月在一次俄国博物学家会议上发表的《论互助的法则》（On the Law of Mutual Aid）这篇演说，却深深地打动了我，使我认为是对整个问题的一个新的启发。凯士勒认为，在大自然中，除了互争的法则以外，还有互助的法则，而这个法则，对生存竞争的胜利，特别是对物种的逐步进化来说，比互争的法则更为重要得多。这种见解——实际上它是达尔文本人在《人类的起源》（The Descent of Man）中所表明的思想的进一步发展——我觉得是如此的正确，如此的重要，所以，自从我（在1883年）知道它以后，就开始搜集材料，以便进一步发挥凯士勒仅仅在他的讲演中泛泛谈到而生前未及加以发挥的思想。他死于1881年。&lt;/p&gt;

&lt;p&gt;只有一点我不完全赞同凯士勒的意见。凯士勒把“亲族感”和对子孙的关心（见第一章）说成是动物之间互相扶助的根源。然而，要断定这两种情感在合群的本能的进化中真正起了多大作用，以及其他本能在同一个方面起了多大作用，我觉得完全是另外一个十分广泛的问题，这个问题我们现在还很难讨论。只有当我们很好地证实了各纲动物中间的互助事实和互助对进化的重要性以后，我们才能研究在合群感的进化中，哪些是属于亲族感的，哪些是属于固有的合群性的——后者显然是起源于动物世界的进化的最初阶段，甚至是在“群体时期”。因此，我把主要注意力首先放在证明互助因素在进化中的重要性上，而把寻求互助本能在自然中的起源这个工作留待以后研究。&lt;/p&gt;

&lt;p&gt;互助这个要素的重要性——“只要它的普遍性能表现出来”——是不能不受到天才的博物学家歌德的注意的。艾克尔曼有一次（在1827年）告诉歌德说，从他那里飞走的两个小鹪鹩，第二天他在知更鸟的窠中找到了，老知更鸟给这两个小鹪鹩和它自己的小鸟一同喂食。歌德听到这件事以后十分兴奋，他认为这证实了他的泛神论，他说：“如果把食物给陌生者吃的这种事实果真象具有普通法则性质的事物一样存在于整个大自然中，那么，许多谜都可以得到解释了。”他第二天又谈起这件事，并且极为诚恳地要求艾克尔曼（大家都知道他是一个动物学家）特别研究一下这个问题，并且说他一定能获得“不可估价的成果”（《对话》，1848年，第3卷，第219、221页）。可惜这项研究始终没有人来做，虽然布利姆很可能是受了歌德这句话的启发才在他的著作中对动物之间的互助收集了那样丰富的材料。&lt;/p&gt;

&lt;p&gt;1872—1886年间出版了几本论述动物的智慧和精神生活的重要著作（这些著作已列举在本书第一章的脚注中），其中三本专门讨论我们所研究的问题的是：伊士比纳的《动物社会》（Les Sociétés animales，巴黎，1877年）、拉纳桑的一篇讲稿《为生存而竞争和为竞争而团结》（La Lutte pour l’existence et l’association pout la lutte，1881年4月）以及路易·彼希纳的《动物世界的爱和爱情生活》（Liebe und Liebes-Leben in der Thierwelt）。后一本书的第一版发表于1882年或1883年，第二版增加了许多材料，发表于1885年。虽然这几本书每本都很出色，但仍需要用大量的篇幅来阐述互助不仅是道德本能起源于人类以前的论据，而且还应作为一个自然法则和进化的要素来考虑。伊士比纳所着重研究的主要是按照生理分工构成的动物社会（例如蚂蚁和蜜蜂的社会），虽然他的著作对一切可能谈到的方面都是很好的提示，但它是在人类社会的进化还不能以我们现在所具有的知识来研究的时候写的。拉纳桑的讲稿更近似一篇层次分明、循序渐进的工作总纲，它从海中的岩石进而谈到植物、动物和人类的世界，这样来论述互助。至于彼希纳的著作，虽然能给人以启发和列举了大量的事例，但是我不能赞同它的主要观点。这本书一开始就赞美爱，它所有的例证几乎都是用来证明在动物之间存在着爱和同情的。然而，把动物的合群性降低为爱和同情，就等于是降低它的普遍性和重要性，正如以爱和个人同情为基础的人类伦理学只能缩小整个道德感的意义一样。当我看见邻居的屋子着火时，使我提着一桶水跑去救火的并不是我对我的邻居（我和他素不相识）的爱，而是更为广泛的（虽说比较模糊）人类休戚相关和合群的本能或情感。这在动物中也是一样。使一群反刍动物或马围成一圈以抵抗狼群攻击的，不是爱，甚至也不是（按本来意义来理解的）同情；使狼成群猎食的不是爱，使小猫或羊羔在一起嬉戏的，或者使十几种小鸟在秋天里聚在一起生活的也不是爱；使散布在像法国那样广大土地上的𪊥结成几十个单独的群，共同走向一定的地点，以便在那里渡过一条河流的，既不是爱，也不是个体间的同情。那是比爱或个体间的同情不知要广泛多少的一种情感——在极其长久的进化过程中，在动物和人类中慢慢发展起来的一种本能，教导动物和人在互助和互援的实践中就可获得力量，在群居生活中就可获得愉快。&lt;/p&gt;

&lt;p&gt;这个区别的重要性，是动物心理学家容易理解的，而且是研究人类伦理的人们更容易理解的。爱、同情和自我牺牲，在我们的道德感的逐步进化中肯定起了巨大作用。但是社会在人类中的基础，不是爱，甚至也不是同情，它的基础是人类休戚与共的良知——即使只是处于本能阶段的良知。它是无意识地承认一个人从互助的实践中获得了力量，承认每一个人的幸福都紧密依赖一切人的幸福，承认使个人把别人的权利看成等于自己的权利的正义感或公正感。更高的道德感就是在这个广泛而必要的基础上发展起来的。但这个问题不属于本书讨论的范围，在这里，我只提出我在答复赫胥黎的《伦理学》（Ethics）时所发表的《正义和道德》（Justice and Motality）这篇演说，我在这篇演说中较详细地谈到了这个问题。&lt;/p&gt;

&lt;p&gt;因此，我认为写作本书来阐述互助为一个自然法则和进化的要素，也许可以弥补一个重大的空白。当赫胥黎在1888年发表他的“生存竞争”宣言（《生存竞争和它对人类的意义》，Struggle for Existence and Its Bearing upon Man）时，我认为它对于人们在灌木丛和森林中所见到的自然界事实，陈述得很不正确，于是，我便和《十九世纪》（Nineteenth Century）杂志的编者洽商，问他可否让我在他的刊物上详细回答一个最杰出的进化论者的意见，编辑詹姆斯·诺耳斯先生慨然接受了这个建议。我对贝茨也谈起过这件事情。“是的，这才是真正的达尔文主义，”他回答说，“‘他们’把达尔文的话弄成那种样子，真是可怕。写这些文章吧，等它们出版的时候，我将写一封信给你，你可以把那封信公开发表。”遗憾的是，我花了将近七年工夫写这些文章，当最后一篇文章发表的时候，贝茨已经逝世了。&lt;/p&gt;

&lt;p&gt;在论述了互助在各纲动物中间的重要性以后，我显然不得不进而讨论这个要素在人类的进化中的重要性。讨论这一点，是更有必要的，因为有许多进化论者也并不否认互助在动物之间的重要性，但是他们，例如赫伯特·斯宾塞，却不承认它对人类的重要性。他们认为，对原始人来说，个人对整体的斗争是生存的唯一法则。我们在论述蒙昧人和野蛮人这两章中将讨论这个从霍布斯时代起就一再有人不加适当批判便过分地乐于彼此转告的论断，究竟在多大程度上是符合我们所知道的早期人类发展的事实的。&lt;/p&gt;

&lt;p&gt;在人类最初的氏族时期以及（在更大程度上）后来的村落公社时期中，由蒙昧人和半野蛮人的创造天才所发展的互助制度的数目和重要性，以及这些早期制度迄今对人类后来的发展的巨大影响，促使我把我的研究范围也扩展到较后的有史时期，特别是最有兴趣的中世纪的自由共和城邦时期，这个时期对现代文明的普遍性和影响以及普遍存在于现代文明中的情况，还没有为人们所充分了解。最后，我试图简短地指出：人类在极其漫长的进化过程中所继承的互助的本能，就是今日在我们的现代社会中也具有巨大的重要性，这个社会据说是按“人人为自己，国家为大众”的这个原则建立的，但它从来不能而且将来也不能实现这个原则。&lt;/p&gt;

&lt;p&gt;也许有人会对本书表示异议，说书中对动物和人类都是按照过于好的一面来阐述的，太强调了他们的合群性，而对于他们反社会和利己的本能却几乎没有谈到。这是不可避免的。近来我们总是听说“冷酷无情的生存竞争”，据说每一个动物对所有的动物，每一个“野蛮人”对所有的“野蛮人”，每一个文明人对他所有的同胞，都在进行这种竞争——这种说法竟变成了一个信条，所以首先必须以一系列从完全不同的一面表现动物和人类生活的事实来反驳它们。我们需要指出合群的习性在大自然中以及在动物和人类的逐步进化中所具有的巨大重要性：证明它能使动物更好地防御敌人，时常使它们更易于获得食物（冬粮、移居等）和长寿，因而也更易于发展智力；证明它使人类除了获得上述的利益以外，还使他们虽然在历史上历经沧桑，但仍能建立种种组织，使他们在对大自然的艰苦斗争中能够生存下去和取得进步。这是一本论述互助法则的书，它把互助作为进化的一个主要要素来考察——它所考察的不是所有一切的进化要素和它们各自的价值；必须写了前一本书以后，才可能再写以后的书。&lt;/p&gt;

&lt;p&gt;对个人的自我维护在人类进化中所起的作用，我肯定从未低估过。可是我认为，这个问题需要比以往更加深入地研究。在人类历史上，个人的自我维护过去是而且现在仍然是和又渺小又愚昧的狭隘心地——许多作家认为这就是“个人主义”和“自我维护”——完全不相同的，它要伟大得多和意义深刻得多。创造历史的，也不只是历史学家所说的那些英雄。因此，如果情况许可的话，我准备对个人的自我维护在人类的逐步进化中所起的作用另作讨论。在这里，我只能一般地谈一谈如下的意见：当互助的组织——部族、村落公社、行会和中世纪城市——在历史的进程中开始失去它们原有的特性，开始为寄生体所侵害、从而变成进步的障碍时，个人反抗这些组织的行为往往表现为两个不同的方面。一部分人起来奋力纯洁旧的组织，或创立一个以同一互助原则为基础的更高级的社会；例如，他们试图以“赔偿”的原则来代替“复仇法则”，以后又以对罪行的宽恕或在人类的良心之前人人平等这个更高的理想，来代替按照阶级价值作出的“赔偿”。但是，与此同时，又有另一部分反对蜕化组织的人致力于破坏互助的保护组织，其目的无非是要增加他们自己的财富和极力。在这两种反抗的人们和支持现存组织的人之间的三角斗争中，存在着真正的历史悲剧。但是，要描述这场斗争和真实地研究这三种力量当中的每一种在人类进化中所起的作用，至少需要我写作这本书所花费的那样多的时间。&lt;/p&gt;

&lt;p&gt;在我论述动物之间的互助的那些文章发表以后，又出现了一些探讨这个问题的著作，其中我要提出的是亨利·德鲁蒙德的《罗威尔讲座：人类的上进》（The Lowell Lectures on the Ascent of Man，伦敦，1894年）和苏瑟兰的《道德本能的起源和成长》（The Origin and Growth of the Moral Instinct，伦敦，1898年）。这两本著作，主要都是按照彼希纳的《动物世界的爱和爱情生活》这本书的论点写作的，在第二本著作中，父母情感和家族感被作为在道德感的发展中唯一起作用的因素详加讨论。按照相似的论点研究人类的第三本著作，是吉汀斯教授的《社会学原理》（The Principles of Sociology），这本书的第一版于1896年印行于纽约和伦敦，而它的主要思想，作者在1894年发表的一本小册子中已经作了概括的叙述。然而，要讨论这些著作和我的著作之间的接触以及相似或分歧之处，这项工作我必须留给评论家们去作了。&lt;/p&gt;

&lt;p&gt;本书的各章最初发表于《十九世纪》杂志上（《动物之间的互助》发表于1890年9月和11月；《蒙昧人之间的互助》发表于1891年4月；《野蛮人之间的互助》发表于1892年1月；《中世纪城市中的互助》发表于1894年8月和9月；《我们现代人之间的互助》发表于1896年1月和6月）。我在把它们编辑成单行本时，首先是想把在杂志论文中不得不省略掉的大量材料和关于几个次要之点的讨论汇为一个附录。可是，看来这个附录将使本书的篇幅增加一倍，所以又只好放弃，或者至少是暂时不发表它。现在的附录只包括过去几年中科学界所争论的几个问题；在正文中，我只增补了那种不必改动本书的结构就可加进去的材料。&lt;/p&gt;

&lt;p&gt;我愿借这个机会表达我对《十九世纪》的编者詹姆斯·诺耳斯先生的最衷心感谢，感谢他一知道这些文章的大意后就慨然许诺在他的刊物上发表，并且允许我把它们出版刊行。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;1902年于肯德郡的布隆里&quot;&gt;1902年于肯德郡的布隆里&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;结论&quot;&gt;结论&lt;/h3&gt;

&lt;p&gt;现在，如果我们把从现代社会分析中所能取得的教训同有关互助在动物世界和人类进化中的重要性的许多例证联系起来，我们就可以把我们的研究总结如下。&lt;/p&gt;

&lt;p&gt;我们已经知道，在动物界中绝大多数的种是过群居生活的，它们的联合就是它们在生存竞争中的最好武器，当然，我们所说的生存竞争，是按照达尔文的广义观念来理解的——不是为了取得单纯的生存手段，而是为了抵抗一切不利于物种的自然条件的竞争。凡是把个体间的竞争缩减到最小限度，使互助的实践得到最大发展的动物的种，必定是最昌盛、最能不断进步的。在这种情况下所取得的互相保护以及达到长寿和积累经验的可能性、高度的智力发达和合群习惯的进一步培养，保证了物种的保持，保证了它的扩展和逐步进化。反之，不合群的种是注定要衰退的。&lt;/p&gt;

&lt;p&gt;其次谈到人类，我们知道，人类在石器时代的初期是结成氏族和部落生活的。我们发现，在较低级的蒙昧人阶段，在氏族和部落中就已经产生了一系列的社会组织：我们知道，最早的部落的风俗习惯是人类各种制度的胚胎，而这些制度在以后便成了不断进步的主导力量。从蒙昧人的部落中，成长了野蛮人的村落公社，而一系列新的、范围更广泛的社会风俗习惯和制度，按照在村民议会的管理下共同占有和保卫一定地区的原则，在属于或假定属于一个血统的村落联盟中发展起来，而且迄至今日，这些风俗习惯和制度有许多仍在我们当中存在着。当新的要求引导人类再向前发展时。他们便从城市——地域单位（村落公社）——和行会相结合的双重组织开始，而行会的产生，是由于共同从事一定的职业或技术，或者是为了达到互助和互卫的目的。&lt;/p&gt;

&lt;p&gt;最后，七，八两章列举的事实表明：按照罗马帝国的形式建立起来的国家虽然猛烈地破坏了中世起的各种互助制度，但是，这种新的文明局面是不能持久的。以分散的个人结合为基础的、企图作为人们唯一联合的连锁的国家，没有达到它的目的。互助的倾向终于冲破了国家的无情统治，重又抬起头来，在无数的组合中显示了它的作用。现在，这些组合势将包括生活的各个方面，占有为人类生活和生活耗费物资的再生产所必需的一切。&lt;/p&gt;

&lt;p&gt;也许会有人说，互助虽然是进化的因素之一，然而它所包括的只是人类关系的一个方面；和这个潮流（虽说它有很大的力量）同时存在的，在现在和过去都还有另外一个潮流——个人的自我维护。这种个人的自我维护，不仅表现为个人努力于取得他自己或他的阶级在经济、政治和精神方面的优越地位，而且还起了一个十分重要（虽然不太显著）的作用，那就是它粉碎了部落、村落公社、城市和国家强加于个人的束缚，而这种束缚往往是易于固定不变的。换句话说，个人的自我维护应被看作是一个进步的因素。&lt;/p&gt;

&lt;p&gt;很明显，除非对这两个主要的潮流都加以分析，否则对进化的看法就不可能十分全面。个人的或者个人的集团的自我维护，他们为了夺取优越地位而进行的竞争，以及因此而产生的冲突，已经有人分析、评述过了，而且从远古的时代起就受到人们的颂扬。事实上，一直到现在，只有这一潮流受到叙事诗人、编年史家、历史学家和社会学家的注意。迄至今日所写的历史，几乎完全是记述神权政治、军事权力、专制政治以及以后的富人阶级政权的促进、建立和维持其统治所采用的方法和手段的。这些势力之间的斗争实际上成了历史的主要内容。因此，我们在人类的历史上自然只是看到个人的作用，虽然按照方才所说的论点对这个问题也有重新研究的余地。而在另一方面，互助这一因素迄今完全为人们所忽视了，当代的和过去的著作家竟干脆对它加以否定，甚至还加以嘲笑。因此，首先指出这个因素在动物界和人类社会进化中所起的巨大作用，是十分必要的。只有充分认识到这一点以后，才有可能在这两个因素之间进行比较。&lt;/p&gt;

&lt;p&gt;即使是用多少有些统计性方法来约略地估计一下它们的相对重要性，也显然是不可能的。我们都知道，单单是一次战争在当时和以后所造成的罪恶，就可能超过互助这个原则几百年无休止的活动所造成的善举。但是，当我们发现在动物界中进步的发展和互助是齐头并进的，而物种内部的竞争则是和倒退的发展相伴随的，当我们注意到，就人类来说，甚至在竞争和战争中所取得的胜利也是和每一个进行冲突的国家、城市、党派和部落中的互助的发展成比例的，而且，在进化的过程中战争本身（只要它这样进行）也是为国家、城市或氏族内部的互助达到进步的目的服务的，这时我们便可看出，作为进步的一个因素的互助具有压倒一切的影响力量。我们也知道，互助的实践和它的连续发展，创造了人类能在其中发扬其艺术、知识和智慧的社会生活条件。以互助倾向为基础的制度获得最大发展的时期，也就是艺术、工业和科学获得最大进步的时期。实际上，对中世纪城市和古希腊城市的内部生活加以研究，就可揭示出这样的事实：当行会和希腊氏族内部实行互助和联盟原则赋予个人和集体的巨大主动性结合起来的时候，便给人类带来了历史上的两个最伟大的时期——古希腊的城市和中世纪时期。而在此后历史上的国家时期中上述那些制度的衰退，也正是这两个时期的迅速衰退。&lt;/p&gt;

&lt;p&gt;至于在我们这个世纪工业的突飞猛进，常常有人把它说成是个人主义和竞争的胜利，但是它肯定有一个比这更为深远的根源。一经有了十五世纪的伟大发现，特别是一有了依靠物理学的一系列成就而获得的大气压力的发现，——它们是在中世纪的城市组织之下获得的——蒸汽机的发明以及意味着取得新的动力的革命，必然会随之而来。如果中世纪的城市能够一直存在到把它们的发现用在这一点上，那么，由蒸汽完成的这场革命，其伦理的结果也许是不同的，而同样的革命在技术和科学中也是不可避免地会发生的。的确，现在还有一个没有定论的问题，那就是随着自由城市的崩溃而产生的、在十八世纪前半叶最为显著的工业的普遍衰退，是否曾大大地推迟了蒸汽机和由此而产生的工艺革命的出现。当我们考虑到十二至十五世纪工业在纺织、冶金、建筑和航海方面的惊人发展速度，当我们考虑到这种工业的发展在十五世纪末叶所带来的科学发现，这时我们就必须自问：在中世纪文明衰退以后，在欧洲所发生的艺术和工业的普遍低落，是否推迟了人类对这些成就的充分利用。当然，手艺工人的消灭、大城市的破坏和它们彼此之间关系的断绝，都是不利于工业革命的。我们知道，詹姆斯·瓦特为了使他的发明能够实际应用，花费了他一生中二十多年的工夫，因为他不能在十八世纪时找到在中世纪的佛罗伦萨或布鲁日轻易就能找到的手工业者——他们能够用金属来制造他的机器，并且能够作到蒸汽机所要求的精巧工艺和精确程度。&lt;/p&gt;

&lt;p&gt;因此，如果我们把这个世纪的工业的进步归功于现时所宣称的个人对整体的竞争，就如同不知道下雨原因的人把下雨归功于他所供献给泥偶像的牺牲一样。在工业发展方面，也和其他方面征服自然的行动一样，互助和紧密的联系肯定是、也一向是比互争更有利得多。&lt;/p&gt;

&lt;p&gt;但是互助这一原则的最大重要性，还是在道德方面表现得最充分。互助是我们的道德观念的真正基础，这一点似乎是很清楚的。就互助感情或互助本能的最初根源来说，不论大家的见解如何（不论是把它归之于生物的原因或是超自然的原因），我们必须追溯到动物世界的最低级阶段，我们可以发现它从这一阶段起排除了许多反对的力量，经过人类发展的各个阶段，直到目前都是在不断进步的。甚至不时产生的新宗教——它们总是当互助这一原则在东方的神权国家或专制国家中陷于衰退的时代或者是在罗马帝国崩溃的时期产生的——也只是重申这个原则罢了。这些宗教在卑微的、最低贱的、最受压迫的社会阶层中找到了它们最主要的支持者，在这一阶层中，互助这一原则是日常生活的必要基础。在最初的佛教、基督教和摩拉维亚教派等宗教团体中所采取的新的联合形式，在性质上是回复了早期部落生活中最好的互助形式。&lt;/p&gt;

&lt;p&gt;但是每进行一次恢复这一古老原则的努力，这一原则的基本思想便扩展一次。它从氏族扩展到种族、种族的联盟、民族，最后最低限度在思想上扩展到了整个人类。在扩展的同时，它也更加精深了。在原始的佛教和某督教中。在某些伊斯兰教的先知的著作中，在初期的宗教改革运动中，特别是在十八世纪和我们当代的道德和哲学运动中，人类愈来愈有力地完全抛弃了报复的观念，即“应得的报应”——以善报善、以恶报恶的观念。“勿冤冤相报”和对邻人要厚施薄取这种更崇高的观念，被看作是真正的道德原则，是比单纯的公正、平等或正义这些观念更为优越的原则，更能导致幸福。呼吁于人类的不仅是需要以爱（它永远是个人的，顶多也只是部落的），而且需要以他和每一个人都是一致的这种理解作为行为的指南。因此，我们追溯出我们的伦理观念确实起源于互助的实践（我们在进化的最初阶段就可找到这种实践的痕迹），并且，我们可以断言，在人类道德的进步中，起主导作用的是互助而不是互争。甚至在现今，我们仍可以说，扩展互助的范围，就是我们人类更高尚的进化的最好保证。&lt;/p&gt;</content><author><name>彼得·克鲁泡特金</name></author><summary type="html">不是以横蛮的暴力和诡诈，而是以互助合作来解释生物和社会的进步。</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i.imgur.com/YHHIshI.png" /><media:content medium="image" url="https://i.imgur.com/YHHIshI.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>