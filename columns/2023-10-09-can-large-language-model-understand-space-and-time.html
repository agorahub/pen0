<!DOCTYPE html>
<html lang="en">

	<head>
		<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1" />


	<title>大語言模型可以理解時空？ · The Republic of Agora</title>


<meta name="description" content="有人認為「大語言模型(LLM)可以推論時間與空間問題」。但身為讀過一點心智哲學的人，還是忍不住要提問：「一個東西可以推論時間與空間」究竟是什麼意思？">

<link rel="stylesheet" href="https://agorahub.github.io/pen0/assets/dark.css">
<link rel="icon" href="https://agorahub.github.io/pen0/assets/favicon.png">
<link rel="apple-touch-icon" href="https://agorahub.github.io/pen0/assets/touch-icon.png">
<link rel="stylesheet" href="https://agorahub.github.io/pen0/assets/common.css">

	<link rel="stylesheet" href="https://agorahub.github.io/pen0/assets/post.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.min.js" type="text/javascript"></script>


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="canonical" href="https://agorahub.github.io/pen0/columns/2023-10-09-can-large-language-model-understand-space-and-time.html">
<link rel="alternate" type="application/atom+xml" title="The Republic of Agora" href="https://agorahub.github.io/pen0/feed.xml" />

<!-- Google Font -->
<link href="https://fonts.googleapis.com/css?family=UnifrakturMaguntia" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Libre+Baskerville|Mansalva&display=swap" rel="stylesheet">




  <!-- change your GA id in _config.yml -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-166928354-2', 'auto');
  ga('send', 'pageview');
  </script>


		
	</head>

	<body>

		<div class="head">
  
  <div id="masthead">
    <h3><a href="https://agorahub.github.io/pen0/">The Republic of Agora</a></h3>
  </div>

  
</div>


		<div class="content">
			<article>

  <header>
    <h1>大語言模型可以理解時空？</h1>
    <br>
    <div class="item">
      <img  src="https://i.imgur.com/dD6zGaE.png">
    </div>
    <h3></h3>
    <h4>討厭鬼 | 2023.10.09</h4>
  </header>
  <div class="item">
    <p>有人認為<a href="https://arxiv.org/abs/2310.02207">「大語言模型(LLM)可以推論時間與空間問題」</a>。但身為讀過一點心智哲學的人，還是忍不住要提問：</p>

<p>「一個東西可以推論時間與空間」究竟是什麼意思？</p>

<!--more-->

<p>這個問題內藏了兩個沒說清楚的關鍵概念：</p>

<ol>
  <li>
    <p>「時間與空間」是什麼？</p>
  </li>
  <li>
    <p>「推論」又是什麼？</p>
  </li>
</ol>

<p>物理學認為時空是維度，是事件與物體之間的相對關係。從這個角度來看，正確整理了事件的前後順序與相對距離，的確可以稱為對時空做出了推論。</p>

<p>但問題是，時空不只是一種測量方式。事件需要量測，但我們如何進行量測？我們的心智如何對量測產生預期，進行編碼？</p>

<p>白話地說，我們怎麼知道時空存在？如何知道事件有先後，空間有前後左右上下？如何用這些屬性去整理蒐集到的資料？</p>

<p>哲學認為其中某些問題是無法回答的，尤其因果、時間、空間這類的概念，是我們心智的認知框架，我們在用感官與行動獲得經驗時，就以這些認知框架去「編碼」經驗，故不可能與經驗分開。</p>

<p>白話版，就是我們每個人都帶著因果與時空的有色眼鏡，而且摘不掉。我們所談論的一切經驗，所做的一切測量，都被這種眼鏡染色，被這種框架歸類。</p>

<p>所以LLM可以理解時空，可以做時空的推論嗎？</p>

<p>如果你問的是「LLM能不能將我們獲得的經驗資料，進行整理歸納，使其符合我們認知時空的方式」，答案是當然可以。</p>

<p>這不是因為LLM多厲害，而是因為LLM是一種機器學習，是用模式識別的方式來習得資料之間的相關性。資料背後的脈絡越詳盡，或者資料本身越是發生在某些時間或某些空間中，LLM當然就越能找到該資料與其他資料之間的相關性、相對關係，藉而將資料定位在(人類認為的)正確的時空位置。</p>

<p>但如果你問的是「LLM能不能以我們認知時空的方式，蒐集並整理資料，使其符合我們的整理方式」，我就持保留態度。因為這牽涉到「推論」和「理解」是什麼。</p>

<p>雖然有點骨董，但接下來請容我使用語言學常用的「內涵」與「外延」。我們叫機器「理解」一個東西時，通常是要機器掌握那個東西在世界中適用於那些事物，不適用於那些事物，以及那個東西與其他東西之間如何互動。這是語言學所謂語意的「外延」，是事物與行動之間的互動關係。</p>

<p>但幾乎沒有人會說語意只等於外延，更是幾乎不會有人說人類完全是靠外延來了解語意。我們在學習與使用語言時，還會有一種難以說清的，心裡覺得什麼就是什麼的標準。我們覺得「蘋果」是被賈伯斯咬了一口的3C產品(喂)，「肯德基」是一家很會做蛋塔的公司(一定是這樣!)，「哲學」是一種只會把事情講複雜毫無屁用的空談(這篇是很好的例子。只有讀哲學的人才會自以為是地寫這麼無聊的文章)。</p>

<p>這些標準就叫做「內涵」，是我們使用語言和概念時的指引，很多時候甚至是唯一的指引。我自己通常就是根據內涵來行動或思考，而非先去查詢每個概念或語詞的意義，我猜絕大多數人應該也一樣。</p>

<p>但問題是，內涵是怎麼來的？這個問題就牽涉到「什麼是理解」的關鍵。</p>

<p>只要想一下應該就會發現，內涵不可能單純來自學習外延。即使不管這會造成循環定義，光是「要把哪些外延納入內涵來學習」就會遇到問題。番茄是一種蔬菜嗎？土星是一種湯嗎？次方是一種運算符號嗎？香蕉是一種漿果嗎？中華民國是國家嗎？</p>

<p>(給瘋狂的克蘇魯信徒：以上答案都是「YES」)</p>

<p>但如果內涵不只來自學習外延，內涵哪來的？來自人類的理解。</p>

<p>哲學普遍認為人類與大多動物具有「意向性」，也就是以自己出發，指涉各種事物的能力。我們可以指涉披薩上出現的鳳梨、高壓電線上倒掛的風箏、不斷下降的股價、甚至抽象的數字或流程。</p>

<p>我們可以指涉土星，可以指涉湯，所以當有人說土星是一種湯，我們會覺得對方真是聰明絕妙(最好是啦！)</p>

<p>但LLM有這種能力嗎？訓練LLM的方式應該是沒有，但被訓練成LLM的電腦有沒有？</p>

<p>我不知道，但目前好像還沒看到跡象。有任何線索還請分享，我超有興趣。</p>

<p>這能力在我們討論「時空」、「因果」這類概念時，會特別重要。</p>

<p>因為如果至今的哲學理解是對的，時空與因果的認知框架，就比任何概念和經驗都更早存在。我們用這種框架去跟世界互動，去指涉所有經驗、概念、事物，去蒐集每一筆資料以及賦予資料意義。因此，人類都不知道除了用這種方法以外，關於時空與因果的資料還能如何整理。</p>

<p>但LLM似乎沒有這種認知框架，本身似乎也沒有指涉事物的意向性。它要如何整理關於時空的資料，或者要如何理解人類世界的整理框架？</p>

<p>我不知道它可不可以，也不知道如果它可以的話將會怎麼做。所以在這部分暫且保留。</p>

<p>請注意，這並不表示LLM整理資料的時空屬性時會「出錯」，因為時空屬性是人貼到資料上的，以及人根據時空框架蒐集資料時產生的，前者已經帶了框架，後者不但帶了框架而且更是relational，LLM要識別模式應該沒什麼問題。</p>

<p>而且絕大多數的資料問題，都可以用靠著這種識別模式與脈絡的方式來解決。LLM的處理能力應該會相當強大，相當可信。</p>

<p>但我們得注意，這些都只是「處理」資料，而非「認知」或「推論」那些資料。認知與推論不只是根據既有編碼的方式來編碼資料，也不只是根據既有概念之間的相關性(也就是概念的使用脈絡)來調整每個概念之間的連結與距離。認知與推論經常需要使用語言的內涵，經常必須指涉對象，很可能會需要意向性。</p>

<p>在我們還沒發現LLM或任何形式的電腦，有能力指涉事物，有先驗的框架去整理資料之前，我們最好不要讓電腦獨立去進行我們稱為「推論」或「認知」的事情，或者至少不要把它們處理的結果太當真，因為很可能許多「見人類所未見」的強大處理結果，最後都只是幻覺。</p>

<p>(這在某些領域可能相當重要，例如犯罪、情蒐、金融、總體經濟。因果與時空的推論錯誤可能造成嚴重的傷害)</p>

<p>但電腦即使不具備先驗的認知框架與指涉事物的能力，依然可以準確地根據人類至今以來認知框架建構的龐大資料庫，去進行衍伸運算。這會像是人類(而且是全體人類)獲得了很強大的影分身能力，能夠以更快的速度更廣的範圍去蒐集資料、處理蒐集到的資料。許多過去需要太長時間太多人力去做的整理與運算，未來很可能都能做。</p>

<p>所以我至今依然認為，在電腦還沒有發展出一些至今難以解釋的認知功能(意向性、主體性、qualia等等)之前，我們在設計電腦的功能與服務時，都應該盡量讓電腦與人類互補，成為人類的衍伸，去擴大人類的能力(在這方面主要是記憶、資料整理、語言協助與轉換能力)，而非試圖與人類競爭。讓電腦去「模仿」人類的能力，不僅會破壞人類的經濟環境因而不道德，更會因為搞錯了「認知」是什麼而事倍功半，徒然耗費能源製造汙染。</p>

<!--END-->

  </div>

</article>

		</div>

	</body>

	
	
	<p class="love">
		Made with <i class="fa fa-heart"></i> by <a href="https://github.com/agorahub">Agora</a>
		<button class="hidden scheme"><i class="toggle d-adjust"></i></button>
	</p>
	
	<script src="https://agorahub.github.io/pen0/assets/auto-dark.js"></script>

</html>
